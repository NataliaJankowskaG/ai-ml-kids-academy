{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPUzzlV6NaTG5nR8JuwHveK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NI3XRXXVE2z0","executionInfo":{"status":"ok","timestamp":1750922225211,"user_tz":-120,"elapsed":9717,"user":{"displayName":"Natalia Jankowska","userId":"05036657743218915188"}},"outputId":"ed5cef6d-06ba-428f-8294-56cf29d72384"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import yfinance as yf\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","import os\n","import joblib # Para guardar y cargar el scaler\n","\n","\n","# --- 1. Lista de tickers del IBEX 35 ---\n","IBEX35_TICKERS = [\n","    \"ACX.MC\", \"ACS.MC\", \"AENA.MC\", \"ALM.MC\", \"AMS.MC\", \"BBVA.MC\", \"BKT.MC\",\n","    \"CABK.MC\", \"CLNX.MC\", \"COL.MC\", \"ELE.MC\", \"ENG.MC\", \"FER.MC\",\n","    \"GRF.MC\", \"IAG.MC\", \"IBE.MC\", \"IDR.MC\", \"ITX.MC\", \"MAP.MC\",\n","    \"MEL.MC\", \"MRL.MC\", \"MTS.MC\", \"NTGY.MC\", \"RED.MC\", \"REP.MC\",\n","    \"ROVI.MC\", \"SAB.MC\", \"SAN.MC\", \"SCYR.MC\", \"SLR.MC\", \"TEF.MC\",\n","    \"TL5.MC\", \"UNI.MC\", \"VCX.MC\", \"ASC.MC\", \"PUIG.MC\", \"APAM.MC\"\n","]\n","\n","\n","# --- 2. Función para obtener datos de múltiples tickers ---\n","def get_historical_data_for_training_multiple(ticker_list, start_date, end_date):\n","    \"\"\"\n","    Descarga datos históricos de múltiples tickers de Yahoo Finance y los consolida.\n","    \"\"\"\n","    all_data = pd.DataFrame()\n","    for ticker_symbol in ticker_list:\n","        print(f\"Descargando datos para: {ticker_symbol}...\")\n","        try:\n","            ticker = yf.Ticker(ticker_symbol)\n","            data = ticker.history(start=start_date, end=end_date, interval=\"1d\")\n","            if not data.empty:\n","                data['Ticker'] = ticker_symbol\n","                # reset_index() convierte el índice 'Date' en una columna normal\n","                all_data = pd.concat([all_data, data[['Close', 'Ticker']].reset_index()])\n","            else:\n","                print(f\"Advertencia: No se encontraron datos para {ticker_symbol} en el rango.\")\n","        except Exception as e:\n","            print(f\"Error al obtener datos para {ticker_symbol}: {e}\")\n","\n","    if all_data.empty:\n","        print(\"ERROR: No se pudo recolectar datos de ningún ticker.\")\n","        return None\n","\n","    all_data.sort_values(by=['Date', 'Ticker'], inplace=True)\n","    return all_data\n","\n","# --- 3. Crear secuencias X, y a partir de datos escalados ---\n","def create_sequences(data_scaled, look_back):\n","    \"\"\"\n","    Crea secuencias X (entrada) y y (salida) para la RNN.\n","    \"\"\"\n","    X, y = [], []\n","    for i in range(len(data_scaled) - look_back):\n","        X.append(data_scaled[i:(i + look_back), 0])\n","        y.append(data_scaled[i + look_back, 0])\n","    return np.array(X), np.array(y)\n","\n","# --- 4. Script Principal para Entrenamiento ---\n","if __name__ == \"__main__\":\n","    print(\"Iniciando el entrenamiento del modelo RNN con múltiples tickers...\")\n","\n","    # --- Configuración de datos y modelo ---\n","    START_DATE = \"2010-01-01\" # Rango amplio para un entrenamiento robusto\n","    # Se usa la fecha actual de ejecución en Colab como fin de los datos históricos.\n","    END_DATE = pd.to_datetime('today').strftime(\"%Y-%m-%d\")\n","    LOOK_BACK = 10 # Cuántos pasos de tiempo anteriores usar para predecir el siguiente\n","    RNN_UNITS = 50 # Número de unidades en la capa SimpleRNN\n","    EPOCHS = 200 # Aumentamos las epochs, pero usaremos EarlyStopping\n","    BATCH_SIZE = 64 # Tamaño del lote\n","    VALIDATION_SPLIT = 0.2 # 20% de los datos más recientes para validación\n","\n","    MODEL_FILENAME = \"rnn_ibex_predictor_v2.h5\"\n","    SCALER_FILENAME = \"scaler_ibex_predictor_v2.pkl\"\n","\n","    # --- Configuración de la ruta de guardado para Google Drive ---\n","    SAVE_DIRECTORY = \"/content/drive/MyDrive\" # <--- ¡CAMBIO AQUÍ!\n","\n","    # Crear la carpeta si no existe (aunque para la raíz de Drive es redundante una vez montado)\n","    os.makedirs(SAVE_DIRECTORY, exist_ok=True)\n","    print(f\"Los modelos se guardarán en: '{SAVE_DIRECTORY}'\")\n","\n","    # 1. Obtener los datos históricos de todos los tickers\n","    all_raw_data = get_historical_data_for_training_multiple(IBEX35_TICKERS, START_DATE, END_DATE)\n","\n","    if all_raw_data is None or all_raw_data.empty:\n","        print(\"No se pudo proceder con el entrenamiento debido a la falta de datos consolidados.\")\n","    else:\n","        # 2. Entrenar un único scaler con TODOS los precios de cierre combinados\n","        prices_for_scaler_fit = all_raw_data['Close'].values.reshape(-1, 1)\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        scaler.fit(prices_for_scaler_fit)\n","\n","        # 3. Preparar los datos y crear secuencias para cada ticker usando el scaler global\n","        X_combined, y_combined = [], []\n","\n","        for ticker_symbol in all_raw_data['Ticker'].unique():\n","            ticker_data_raw = all_raw_data[all_raw_data['Ticker'] == ticker_symbol].sort_values(by='Date')['Close'].values.reshape(-1, 1)\n","\n","            # Transformar los datos del ticker usando el scaler GLOBAL\n","            ticker_data_scaled = scaler.transform(ticker_data_raw)\n","\n","            # Crear secuencias (X) y etiquetas (y) para este ticker\n","            if len(ticker_data_scaled) >= LOOK_BACK + 1:\n","                X_ticker, y_ticker = create_sequences(ticker_data_scaled, LOOK_BACK)\n","                X_combined.extend(X_ticker)\n","                y_combined.extend(y_ticker)\n","            else:\n","                print(f\"Advertencia: Datos insuficientes para {ticker_symbol} para crear secuencias con look_back={LOOK_BACK}. ({len(ticker_data_scaled)} puntos)\")\n","\n","        X_combined = np.array(X_combined)\n","        y_combined = np.array(y_combined)\n","\n","        if X_combined.shape[0] == 0:\n","            print(\"ERROR: No se pudieron crear suficientes secuencias de datos para el entrenamiento después del preprocesamiento.\")\n","        else:\n","            # Reshape para Keras: [muestras, timesteps, características]\n","            X_combined = np.reshape(X_combined, (X_combined.shape[0], X_combined.shape[1], 1))\n","\n","            # 4. Dividir datos en entrenamiento y validación (manteniendo el orden temporal)\n","            split_index = int(len(X_combined) * (1 - VALIDATION_SPLIT))\n","            X_train, X_val = X_combined[:split_index], X_combined[split_index:]\n","            y_train, y_val = y_combined[:split_index], y_combined[split_index:]\n","\n","            print(f\"Datos de entrenamiento: {X_train.shape[0]} secuencias\")\n","            print(f\"Datos de validación: {X_val.shape[0]} secuencias\")\n","\n","            # 5. Construir y compilar el modelo RNN\n","            model = Sequential()\n","            model.add(SimpleRNN(units=RNN_UNITS, activation='relu', input_shape=(LOOK_BACK, 1)))\n","            model.add(Dense(units=1))\n","            model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","            # 6. Callbacks para el entrenamiento (EarlyStopping)\n","            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","            print(f\"Entrenando el modelo con {X_train.shape[0]} secuencias...\")\n","            history = model.fit(X_train, y_train,\n","                                epochs=EPOCHS,\n","                                batch_size=BATCH_SIZE,\n","                                validation_data=(X_val, y_val),\n","                                callbacks=[early_stopping],\n","                                verbose=1)\n","\n","            print(f\"Entrenamiento completado. Pérdida final (entrenamiento): {history.history['loss'][-1]:.4f}\")\n","            print(f\"Mejor pérdida de validación: {min(history.history['val_loss']):.4f}\")\n","\n","            # 7. Guardar el modelo entrenado y el scaler en Google Drive\n","            model_save_path = os.path.join(SAVE_DIRECTORY, MODEL_FILENAME)\n","            model.save(model_save_path)\n","            print(f\"Modelo guardado en: '{model_save_path}'\")\n","\n","            scaler_save_path = os.path.join(SAVE_DIRECTORY, SCALER_FILENAME)\n","            joblib.dump(scaler, scaler_save_path)\n","            print(f\"Scaler guardado en: '{scaler_save_path}'\")\n","\n","    print(\"Proceso de entrenamiento finalizado.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CtK4IWEFkfs","executionInfo":{"status":"ok","timestamp":1750922712045,"user_tz":-120,"elapsed":128306,"user":{"displayName":"Natalia Jankowska","userId":"05036657743218915188"}},"outputId":"e253dfb3-5f3a-400a-aef7-b57e26b4cdcd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Iniciando el entrenamiento del modelo RNN con múltiples tickers...\n","Los modelos se guardarán en: '/content/drive/MyDrive'\n","Descargando datos para: ACX.MC...\n","Descargando datos para: ACS.MC...\n","Descargando datos para: AENA.MC...\n","Descargando datos para: ALM.MC...\n","Descargando datos para: AMS.MC...\n","Descargando datos para: BBVA.MC...\n","Descargando datos para: BKT.MC...\n","Descargando datos para: CABK.MC...\n","Descargando datos para: CLNX.MC...\n","Descargando datos para: COL.MC...\n","Descargando datos para: ELE.MC...\n","Descargando datos para: ENG.MC...\n","Descargando datos para: FER.MC...\n","Descargando datos para: GRF.MC...\n","Descargando datos para: IAG.MC...\n","Descargando datos para: IBE.MC...\n","Descargando datos para: IDR.MC...\n","Descargando datos para: ITX.MC...\n","Descargando datos para: MAP.MC...\n","Descargando datos para: MEL.MC...\n","Descargando datos para: MRL.MC...\n","Descargando datos para: MTS.MC...\n","Descargando datos para: NTGY.MC...\n","Descargando datos para: RED.MC...\n","Descargando datos para: REP.MC...\n","Descargando datos para: ROVI.MC...\n","Descargando datos para: SAB.MC...\n","Descargando datos para: SAN.MC...\n","Descargando datos para: SCYR.MC...\n","Descargando datos para: SLR.MC...\n","Descargando datos para: TEF.MC...\n","Descargando datos para: TL5.MC...\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:yfinance:$TL5.MC: possibly delisted; no timezone found\n"]},{"output_type":"stream","name":"stdout","text":["Advertencia: No se encontraron datos para TL5.MC en el rango.\n","Descargando datos para: UNI.MC...\n","Descargando datos para: VCX.MC...\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:yfinance:HTTP Error 404: \n","ERROR:yfinance:$VCX.MC: possibly delisted; no timezone found\n","ERROR:yfinance:$ASC.MC: possibly delisted; no timezone found\n"]},{"output_type":"stream","name":"stdout","text":["Advertencia: No se encontraron datos para VCX.MC en el rango.\n","Descargando datos para: ASC.MC...\n","Advertencia: No se encontraron datos para ASC.MC en el rango.\n","Descargando datos para: PUIG.MC...\n","Error al obtener datos para PUIG.MC: Length mismatch: Expected axis has 2 elements, new values have 1 elements\n","Descargando datos para: APAM.MC...\n","Datos de entrenamiento: 99184 secuencias\n","Datos de validación: 24796 secuencias\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Entrenando el modelo con 99184 secuencias...\n","Epoch 1/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 2.8582e-05 - val_loss: 5.5170e-05\n","Epoch 2/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 1.7981e-06 - val_loss: 4.7369e-05\n","Epoch 3/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.7133e-06 - val_loss: 5.5095e-05\n","Epoch 4/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.5701e-06 - val_loss: 4.5272e-05\n","Epoch 5/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1.5331e-06 - val_loss: 4.5504e-05\n","Epoch 6/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.4597e-06 - val_loss: 4.8870e-05\n","Epoch 7/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.5208e-06 - val_loss: 4.5754e-05\n","Epoch 8/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.4749e-06 - val_loss: 5.3187e-05\n","Epoch 9/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 1.5321e-06 - val_loss: 4.6879e-05\n","Epoch 10/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1.4484e-06 - val_loss: 5.4331e-05\n","Epoch 11/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.5820e-06 - val_loss: 4.7521e-05\n","Epoch 12/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1.3885e-06 - val_loss: 4.7021e-05\n","Epoch 13/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.4046e-06 - val_loss: 5.0158e-05\n","Epoch 14/200\n","\u001b[1m1550/1550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 1.4691e-06 - val_loss: 4.9535e-05\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Entrenamiento completado. Pérdida final (entrenamiento): 0.0000\n","Mejor pérdida de validación: 0.0000\n","Modelo guardado en: '/content/drive/MyDrive/rnn_ibex_predictor_v2.h5'\n","Scaler guardado en: '/content/drive/MyDrive/scaler_ibex_predictor_v2.pkl'\n","Proceso de entrenamiento finalizado.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"d8mYo1sbFkcb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S5kg43ZfFkaA"},"execution_count":null,"outputs":[]}]}